{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":184797058,"authorName":"Benjamin Carlyle","from":"Benjamin Carlyle &lt;benjamincarlyle@...&gt;","profile":"fuzzybsc","replyTo":"SENDER","senderId":"1D1sNQCJMMNC97zqOHjbdWbeO9-1U6mdA95YF-UzfQ59UaPfAiKPjTopj2r8kC-Y6PvO5urylYHhf8DJIItZ1_Fby224ZppZN8bbP4a_OyBFZerEBu9WoEc","spamInfo":{"isSpam":false,"reason":"0"},"subject":"Re: The XML Semantic Web","postDate":"1172267771","msgId":7972,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDExNzIyNjc3NzEuNDc1NS42MS5jYW1lbEBsb2NhbGhvc3QubG9jYWxkb21haW4+","inReplyToHeader":"PDQ1REMxMzNGLjEwMjAwMDVAZGVob3JhLm5ldD4=","referencesHeader":"PDExNzE5NjkxNzYuMjA3LjI3NTcyLm0xNkB5YWhvb2dyb3Vwcy5jb20+CSA8NDVEQUQyNjIuMjM4Ny4zRUFBNjg3QGFuZHJ6ZWouY2hhZXJvbi5jb20+CSA8MTE3MjAwNTEwMC4xMjIwMi4xMC5jYW1lbEBsb2NhbGhvc3QubG9jYWxkb21haW4+CSA8NDVEQjdDRTUuNTA1MDFAZGVob3JhLm5ldD4JIDw5Mjc0NDFiMzA3MDIyMDE4MTB1NDQ3Yjk4OWZzYjZlMjBhMGI3MzQ4MmEwNEBtYWlsLmdtYWlsLmNvbT4JIDw0NURDMTMzRi4xMDIwMDA1QGRlaG9yYS5uZXQ+"},"prevInTopic":7965,"nextInTopic":7973,"prevInTime":7971,"nextInTime":7973,"topicId":7927,"numMessagesInTopic":81,"msgSnippet":"... Let me have a quick go at debunking RDF. I have put a few years thought into this. While I know that what I am about to say works directly against deep","rawEmail":"Return-Path: &lt;benjamincarlyle@...&gt;\r\nX-Sender: benjamincarlyle@...\r\nX-Apparently-To: rest-discuss@yahoogroups.com\r\nReceived: (qmail 48657 invoked from network); 23 Feb 2007 21:56:34 -0000\r\nReceived: from unknown (66.218.66.70)\n  by m47.grp.scd.yahoo.com with QMQP; 23 Feb 2007 21:56:34 -0000\r\nReceived: from unknown (HELO mail01.syd.optusnet.com.au) (211.29.132.182)\n  by mta12.grp.scd.yahoo.com with SMTP; 23 Feb 2007 21:56:33 -0000\r\nReceived: from c210-49-75-64.rochd2.qld.optusnet.com.au (c210-49-75-64.rochd2.qld.optusnet.com.au [210.49.75.64])\n\tby mail01.syd.optusnet.com.au (8.13.1/8.13.1) with ESMTP id l1NLuCYe018922;\n\tSat, 24 Feb 2007 08:56:23 +1100\r\nTo: Bill de hOra &lt;bill@...&gt;\r\nCc: rest-discuss@yahoogroups.com\r\nIn-Reply-To: &lt;45DC133F.1020005@...&gt;\r\nReferences: &lt;1171969176.207.27572.m16@yahoogroups.com&gt;\n\t &lt;45DAD262.2387.3EAA687@...&gt;\n\t &lt;1172005100.12202.10.camel@...&gt;\n\t &lt;45DB7CE5.50501@...&gt;\n\t &lt;927441b30702201810u447b989fsb6e20a0b73482a04@...&gt;\n\t &lt;45DC133F.1020005@...&gt;\r\nContent-Type: text/plain\r\nDate: Sat, 24 Feb 2007 07:56:11 +1000\r\nMessage-Id: &lt;1172267771.4755.61.camel@...&gt;\r\nMime-Version: 1.0\r\nX-Mailer: Evolution 2.6.3 \r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:0:0:0\r\nFrom: Benjamin Carlyle &lt;benjamincarlyle@...&gt;\r\nSubject: Re: The XML Semantic Web\r\nX-Yahoo-Group-Post: member; u=184797058; y=OlcmC-S4BI8aH-xnJkEWcuON_BpRozBVkvgVsSGAvcxsvOU\r\nX-Yahoo-Profile: fuzzybsc\r\n\r\nOn Wed, 2007-02-21 at 09:39 +0000, Bill de hOra wrote:\n&gt; Hugh Winkler wrote:\n&gt; &gt; On 2/20/07, Bill de hOra &lt;bill@...&gt; wrote:\n&gt; &gt;&gt; The last part by the way\n&gt; &gt;&gt; about complete document semantics simply isn&#39;t true if your\n&gt; document\n&gt; &gt;&gt; semantics are based on model theoretic semantics - the agents\n&gt; &gt;&gt; communities have been slinging documents back and forth for years\n&gt; &gt;&gt; without been tied to runtimes. It&#39;s just that programming to a\n&gt; model\n&gt; &gt;&gt; theory is esoteric compared to switch-on-type; programmers\n&gt; generally\n&gt; &gt;&gt; won&#39;t roll with it.\n&gt; &gt; Well, yeah. We really need to find a way to make it practical to do\n&gt; &gt; reasoning over these tags. It&#39;s hard because: there are lots of\n&gt; &gt; definitions, the definitions are not centrally located, and even if\n&gt; &gt; you get them all in one place, reasoning over them is a massive\n&gt; &gt; program in itself. So I think to make it practical, programs have to\n&gt; &gt; call a service to do it for them.\n&gt; No argument there. I&#39;d love to read a nutshell analysis as to why\n&gt; people \n&gt; aren&#39;t falling over themselves for those kinds of technologies (kr, \n&gt; rules, agents, semweb) - they directly address multi-billion dollar \n&gt; issues in the industry (specifically integration and change\n&gt; resilience). \n&gt; I&#39;ve always seem application protocols as the &quot;software agent&quot; \n&gt; equivalent of grunting.\n\nLet me have a quick go at debunking RDF. I have put a few years thought\ninto this. While I know that what I am about to say works directly\nagainst deep assumptions of current semantic web proponents, I believe\nit is grounded in reality. I also believe that we need to face these\nissues and come up with some good answers in order to actually achieve\nthe semantic web.\n\nFirstly, let me define the semantic web as I see it:\nThe semantic web is a software architecture in which architecture\ncomponents can initiate or otherwise be involved in standard\ninteractions such as GET requests with corresponding responses. These\ninteractions transfer data from one component to another in standard\nforms that can be understood, correctly interpreted, and used for the\npurposes of the consuming component. Because standard interactions are\nin use, it is possible to configure each component to talk to just about\nany other component that describes its data in the same way. \n\nThe data description comes in three parts:\n* Document type\n* Vocabulary\n* Structure\n\nYou might recognise my semantic web as being the same as the RESTful\nweb. I see the two as strongly correlated.\n\n&gt;From a RESTful perspective we are interested in using standard document\ntypes, which implies the use of standard vocabulary and structure. RDF\ndecouples structure from vocabulary and perhaps from document type,\nsuggesting that many document types on the web should use one of its\nserialisations. Vocabulary should be mapped onto the graph so that it\ncan arbitrarily aggregated without loss of information, then later\nqueried.\n\nI challenge the effectiveness of RDF on a number of points\n* The effectiveness of the graph structure for conveying data machine to\nmachine\n* The importance of aggregation at the graph level\n* The mechanisms for seeing vocabulary evolve, and for mixing\nvocabularies\n\nGraph Structure Effectiveness\nWhen we are talking about pure machine-to-machine integration I see the\nend to end process as follows:\n* Machine 1 has information in an internally-defined structure\n* Machine 1 encodes this information into a representation\n* Machine 2 recieves the representation\n* Machine 2 decodes the representation into its own internal structures\nIn some cases the internal structure will literally be the\nrepresentation, ie a string. In other cases the gap between internal\nstructure and representation will be wider. In very few cases do I think\nwe will see an RDF-like graph as the internal structure on either side\nof this communication.\n\nIn other words, the graph does not add value to machine to machine\ncommunications. Higher-level structured XML documents have proven\nthemselves as more effective. It is easier to encode information to or\nextract information from an atom document than from the equivalent RDF.\nRDF requires more complex model-to-model transformations than the\neasy-to-traverse tree structure of XML. RDF imposes an unnecessary\nburden on both sides of the information exchange that results in more\ncode being written, rather than less.\n\nGraph-level Aggregation\nThe core selling point of RDF seems to be the ability to aggregate\narbitrary information into a single document. In the machine-to-machine\nexample this has no value because the additional information won&#39;t be\nunderstood by the second machine and will be ignored. However, if we\nthrow the data into an RDF triplestore we might be able to extract it\nlater using appropriate SPARQL or other queries.\n\nThis is really the core use case, I think, of RDF. I go and crawl the\nweb and aggregate data that I can later run queries on. In other words,\nit is a way for the google of the semantic web to learn &quot;everything&quot; and\nto have queries run against it.\n\nWhile this might be possible, it relies on a controlled set of\nvocabularies being defined. As I will point out later in the document, I\ndon&#39;t think RDF is as conducive to good vocabulary evolution as XML. The\nuse case is also limited. It doesn&#39;t really impact on the likelyhood\nthat arbitrary components of the architecture will be able to have a\nmeaningful conversation. It just allows particular kinds of search. We\ncan see this with early RSS. RSS was defined in terms of RDF so that it\ncould be easily aggregated. However, aggregation did not happen at the\nRDF level in practice. Instead, RSS was aggregated at a higher level.\n\nAggregation and Evolution\nI am not a fan of XML namespaces. We have a MIME type that defines what\nkind of document we are parsing. My inclination is usually to ignore any\nXML namespace and just rely on the mime to get me home. Some documents\ninclude sub-documents. For example, atom can include xhtml for use in\nsome elements. The containing element indicates to anyone who can\nprocess atom what they should be doing with the content, regardless of\nits type. In this case it is an XML namespace that selects the parser\nfor use in this sub-document. A more uniform scheme would be\npreferrable.\n\nAs well as allowing targetted aggregation, XML can be subclassed.\nMust-ignore semantics mean that a document with additional elements will\nbe ignored by old implementations. This allows new versions of the\ndocument type to be deployed without breaking the architecture. It also\nallows extensions to be added for various purposes. If we continue to\nuse mime we can be specific about particular kinds of subclasses. For\nexample, I might sub-class atom for the special purpose of indicating\nthe next three trains that will arrive at a railway station:\napplication/pids+atom+xml.\n\nRDF isn&#39;t really as flexible. You can include foreign statements for\naggregation, but you can&#39;t easily control what they mean in your\ncontext. RDF statements are intended to be context-free, but even very\ngeneric statements like dc:author are likely to need some assumptions to\nbe made in order to interpret them correctly. They are not part of the\nparent vocabulary so aren&#39;t really part of the parent document type.\n\nIf you want to subclass a vocabulary you are looking at defining your\nown vocabulary in its own namespace that extends the original one. While\nthis might be ok initially, it is as vocabularies evolve that you run\ninto trouble. Once the wider community around a particular vocabulary\nsees my extensions as valuable, how do they move into the standard\nsphere? Do they need to keep the namespace in which they were first\nintroduced?\n\nThere is an argument to be made[1] that whenever the architecture\ndemands that you constrain something (methods, document vocabulary,\ndocument types) that providing an infinite namespacing scheme actually\nhelps different communities avoid conflict and face-offs that they\nshould be having. If html had made it easy for netscape and microsoft to\nput their extensions into their own namespaces, would html still be the\nstrong standard that it is today? Would we have to remember whether it\nwas microsoft or netscape who implemented the blink tag in order to\ninclude it into our documents?\n\nNamespaces should be controlled when the architecture demands\nconstraints in the area they govern. Additionally, the argument is there\nto continue using the main document namespace for any extensions rather\nthan introducing a new namespace, whether they have been ratified by\nanyone or not. Sub-classing the mime type is a good way to avoid\nnamespace conflict in the short term, then as vocabulary and structure\nmove from the special to the general and back again we don&#39;t have to\nkeep track of exactly where terms originated from.\n\nConclusion\n\nRDF does not seem to have the vocubarly evolution mechanisms that XML\ncontent types have available to it. RDF overemphasises the low-value\ngraph model, and undempasises high-value problem-specific structure. I\nthink that the semantic web will not be constructed of a single abstract\nmodel, but one that is built up of solutions to various specific\nproblems. I see a semantic web of document types that include a little\nhtml, structure themselves around atom, and add a number of other\ndocument types into a single structure for good measure. I see a bounded\nnumber of actual document types that are built up in this way so that\ncomponents of the architecture can understand messages that are sent to\nthem, rather than just aggregating their data.\n\nI see the semantic web in terms of machine to machine integration,\nrather than data to database aggregation.\n\nBenjamin.\n[1] http://www.mnot.net/blog/2006/04/07/extensibility\n\n\n"}}