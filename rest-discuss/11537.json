{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":266460716,"authorName":"Eric J. Bowman","from":"&quot;Eric J. Bowman&quot; &lt;eric@...&gt;","replyTo":"SENDER","senderId":"gZFX4pRflJDlZuZ8-CMcumPEwXMBnJ-Dn7LqMGaU0Du3QJiZgvJoOYuijqpjAGC7bYfHx3THebc-ngsuNMAka69nFUp_HXbiRCNfclyeIg","spamInfo":{"isSpam":false,"reason":"6"},"subject":"Re: [rest-discuss] HTML5 and RESTful HTTP in browsers","postDate":"1227164562","msgId":11537,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDIwMDgxMTIwMDAwMjQyLmRhN2IzNTNmLmVyaWNAYmlzb25zeXN0ZW1zLm5ldD4=","inReplyToHeader":"PDQ5MjJGRDgwLjkwNjA5MDZAbXlrYW5qby5jby51az4=","referencesHeader":"PDIwMTUxMDEwLjIzNjg3MTIyNjkxOTk1NDUzMi5KYXZhTWFpbC5zZXJ2bGV0QGt1bmRlbnNlcnZlcj4JPDIwMDgxMTE4MDA1OTQ5LmFiNzQ0YmVkLmVyaWNAYmlzb25zeXN0ZW1zLm5ldD4JPDQ5MjJCMzZBLjMwOTA4MDRAbXlrYW5qby5jby51az4JPDIwMDgxMTE4MDgxMzM2LmYxNTY5ZTg1LmVyaWNAYmlzb25zeXN0ZW1zLm5ldD4JPDQ5MjJGRDgwLjkwNjA5MDZAbXlrYW5qby5jby51az4="},"prevInTopic":11534,"nextInTopic":11550,"prevInTime":11536,"nextInTime":11538,"topicId":11508,"numMessagesInTopic":82,"msgSnippet":"On Tue, 18 Nov 2008 17:38:08 +0000 ... Take your time, conneg is a toughie, re-learning it in the context of REST won t happen overnight... but, bluntly,","rawEmail":"Return-Path: &lt;eric@...&gt;\r\nX-Sender: eric@...\r\nX-Apparently-To: rest-discuss@yahoogroups.com\r\nX-Received: (qmail 3890 invoked from network); 20 Nov 2008 07:02:39 -0000\r\nX-Received: from unknown (66.218.67.96)\n  by m44.grp.scd.yahoo.com with QMQP; 20 Nov 2008 07:02:39 -0000\r\nX-Received: from unknown (HELO mxout-08.mxes.net) (216.86.168.183)\n  by mta17.grp.scd.yahoo.com with SMTP; 20 Nov 2008 07:02:38 -0000\r\nX-Received: from BigBison (unknown [65.117.211.162])\n\t(using TLSv1 with cipher DHE-RSA-AES256-SHA (256/256 bits))\n\t(No client certificate requested)\n\tby smtp.mxes.net (Postfix) with ESMTP id EB860D0563;\n\tThu, 20 Nov 2008 02:02:37 -0500 (EST)\r\nDate: Thu, 20 Nov 2008 00:02:42 -0700\r\nTo: Mike &lt;mike@...&gt;\r\nCc: rest-discuss@yahoogroups.com\r\nMessage-Id: &lt;20081120000242.da7b353f.eric@...&gt;\r\nIn-Reply-To: &lt;4922FD80.9060906@...&gt;\r\nReferences: &lt;20151010.236871226919954532.JavaMail.servlet@kundenserver&gt;\n\t&lt;20081118005949.ab744bed.eric@...&gt;\n\t&lt;4922B36A.3090804@...&gt;\n\t&lt;20081118081336.f1569e85.eric@...&gt;\n\t&lt;4922FD80.9060906@...&gt;\r\nOrganization: Bison Systems Corporation\r\nX-Mailer: Sylpheed 2.4.5 (GTK+ 2.10.14; i686-pc-mingw32)\r\nMime-Version: 1.0\r\nContent-Type: text/plain; charset=US-ASCII\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Msg-Info: 1:6:0:0:0\r\nFrom: &quot;Eric J. Bowman&quot; &lt;eric@...&gt;\r\nSubject: Re: [rest-discuss] HTML5 and RESTful HTTP in browsers\r\nX-Yahoo-Group-Post: member; u=266460716\r\n\r\nOn Tue, 18 Nov 2008 17:38:08 +0000\nMike wrote:\n&gt; \n&gt; &gt; I&#39;ve tried to elaborate here, rather than simplify, hope this helps.\n&gt; &gt;   \n&gt; Not really, no!\n&gt; \n\nTake your time, conneg is a toughie, re-learning it in the context of\nREST won&#39;t happen overnight... but, bluntly, that&#39;s exactly what you\nneed to do before continuing down the @accept path -- forget everything\nyou think you know about resources, representations and content\nnegotiation.  I&#39;m in the mood to write about conneg in general, and\nhope it helps anyone reading this, since it&#39;s really an under-\nelaborated topic.  Plenty of folks here take the attitude of just not\nusing conneg aside from GZIP, but I like it for all sorts of things.\n\n&gt;\n&gt; &gt; Errr, uhhh, a Content-Type is not a representation.  A URI may serve\n&gt; &gt; multiple representations, but there&#39;s no requirement for a 1:1\n&gt; &gt; relationship with MIME type.  A URI may serve HTML 4.01 to some\n&gt; &gt; clients and XHTML 1.0 to others, both as text/html, with content\n&gt; &gt; negotiation based on User-Agent rather than Accept, even.  Or, a\n&gt; &gt; URI may serve two different representations of an HTML 4.01 page,\n&gt; &gt; both served as text/html, one zipped and the other raw, with\n&gt; &gt; content negotiation based on Accept-Encoding.\n&gt; &gt;   \n&gt; \n&gt; There is no practical reason to modify User-Agent in markup, the UA\n&gt; is probably the best indicator of this header, so I don&#39;t see how\n&gt; that is relevant here.\n&gt; \n\nWhen I say content negotiation may be based on User-Agent, that&#39;s a\nserver-side configuration, I&#39;m not sure where you inferred that meant\nmodifying UA in markup on the client.  That aside, just because you\nhaven&#39;t thought of a practical application for something doesn&#39;t mean\nthere isn&#39;t one; this is why I point to XForms 1.1 as an example since\nit allows the author/generator to set any header(s) to any value(s).\n\nJavaScript and XForms allow separation of protocol operations from\nmarkup, whereas @accept would couple protocol together with markup.\nThe principle here is REST&#39;s optional Code on Demand constraint.  I\nthink the notion of overriding a client&#39;s request headers belongs in\nJavaScript or XForms since they&#39;re CoD, not in the core markup language.\n\n&gt;\n&gt; Accept-Encoding; again I can&#39;t think of a practical use for\n&gt; specifying this in markup.\n&gt; \n\nTwo HEAD requests, one with Accept-Encoding: gzip and one without an\nA-E header, might return two different Content-Length values, allowing\nthe client to calculate the compression ratio for a page and its\ninline elements, and report it to the user.  I could do this in either\nJavaScript or XForms, but unless the markup or script overrides the\nclient&#39;s innate Accept-Encoding header, this can&#39;t be done.  Best\nthing for innovation would be to implement a way to set any header(s)\nto any value (s) rather than allowing some and disallowing others.\n\n&gt;\n&gt; &gt; A resource is anything identified by a URI.\n&gt; \n&gt; Is it? I thought that people often misuse URIs to represent other \n&gt; horrible things like.. methods!\n&gt; \n\nWhat I should have said is, &quot;Anything identified by a URI is a\nresource&quot;, what I actually isn&#39;t technically accurate.  But I assure\nyou, all URIs identify resources (even resources that don&#39;t exist),\nthat&#39;s why they&#39;re called resource identifiers...  Using a URI to\nidentify a method specifically violates the Uniform Interface\nconstraint of REST, but it&#39;s still a resource, by definition of URI.\n\n&gt;\n&gt; &gt; In the case of a\n&gt; &gt; negotiated URI with multiple representations, each representation\n&gt; &gt; may be a resource in its own right, you just have to assign it a\n&gt; &gt; URI.\n&gt; \n&gt; I noticed you used the word &#39;may&#39;. It pretty much has to as it\n&gt; stands, unless I use Javascript to control the Accept header in\n&gt; requests.\n&gt; \n\nNo.  If you enable GZIP compression for a resource, you will double the\nnumber of representations, but this won&#39;t assign separate URIs for\nthose representations.  Since neither the zipped representation nor the\nraw (unzipped) representation has its own URI (there is now a URI which\nnegotiates between both representations), they are not resources.\n\n(Maybe I&#39;m wrong there and technically, even without their own URIs\nthey&#39;re still resources by virtue of being representations, but I\nstopped my confusion on that point by pragmatically deciding not to\npersonally call anything a resource unless and until it has a URI.  My\ncoffee cup here is technically a REST resource, but as I&#39;ve not assigned\nit a URI I don&#39;t recognize that it has any rights as a resource.  This\nsounds monstrous of me, I know, but then again who names a coffee cup?)\n\nControlling the Accept header in requests has no effect on this server\nbehavior, but you can have either no Accept-Encoding header, or set\nthat header to &#39;gzip&#39; and a client-side script (or markup) can toggle\nbetween zipped and raw representations on plenty of real-world URIs.\n\nIf for some reason you want the zipped and the raw representations to\nhave their own URIs, then mint them, you now have one zipped resource\nand one raw resource (by definition, once they have URIs), which are\nboth representations of some other negotiated resource.\n\n(Despite being Mozilla-based, the K-meleon browser does not implement\nGZIP.  Compressed content would render in Martian on K-meleon, since\nthere&#39;s no codec for it.  Without content negotiation based on Accept-\nEncoding, the server would have to compress all output, and all clients\nwould have to implement GZIP, or there wouldn&#39;t be compression at all.\nBut, since servers do use content negotiation to implement compression,\nthe server can serve the raw representation to any browser (like\nK-meleon) that doesn&#39;t Accept-Encoding: gzip, and the compressed\nrepresentation to most other browsers, because clients don&#39;t need to\nknow how to negotiate content -- just set their headers properly.)\n\n&gt;\n&gt; I don&#39;t really see the value in distinguishing a representation of a \n&gt; resource as a resource in it&#39;s own right, other than working around \n&gt; insufficiencies in technologies like HTML that prevent developers\n&gt; from indicating where a particular representation of a resource is\n&gt; appropriate.\n&gt; \n\nHTML doesn&#39;t enter into the equation when we&#39;re talking about\ncompressing HTTP output streams.  Content negotiation, and resource vs.\nrepresentation vs. URI *are* the equation.  Until you understand that,\nand therefore understand REST&#39;s resource/representation language,\nyou&#39;re going to have a tough time convincing folks that these concepts\narose due to the limitations of HTML -- let alone the need for adding\nAccept-header tracking to bookmarks.  It isn&#39;t just that I don&#39;t see\nthe need for that, it&#39;s that I fail to see any limit it imposes on\ninnovation.  This is because you haven&#39;t convinced me that there&#39;s any\nactual problem that needs fixing, there -- bookmark Content-Location.\n\nAnyway, HTML only defines the syntax and semantics of a markup\nlanguage, it does not define HTTP headers or how to manipulate them,\nbeyond POST vs. GET and &lt;meta http-equiv&gt;, which isn&#39;t the same thing\nas having control over request headers.  Sprinkling header control\nthroughout a document using an attribute is not an ideal solution, as\nthe resulting markup language becomes coupled to the HTTP protocol,\nwhereas leaving header control in JavaScript or a separate forms module\nwould make it easier to migrate to a new protocol (waka).  Whatever bad\nthings one wishes to say about HTML only including support for GET and\nPOST, it did prove the concept (perhaps due to this limitation) that a\nmarkup language should be portable to new protocols, in moving from\nHTTP 1.0 to 1.1. (Yeah, everyone wants PUT support now, but where the\nhell were all you guys twelve years ago?  ;-)\n\nXForms isn&#39;t a stand-alone language, it&#39;s an HTTP-specific extension\nfor XML markup languages. This, like using JavaScript to manipulate HTTP\nrequest headers, is an example of REST&#39;s optional Code on Demand\nconstraint, so is the client-side XSLT capability of major modern\nbrowsers.  XForms takes a bit of selling to include there, but I&#39;m\nwilling to give it a try if anyone needs me to... I&#39;m aware that it\nisn&#39;t as clear-cut a case as JavaScript or XSLT.\n\nWhat I&#39;m thinking, is that had early HTML adopted more HTTP methods,\nthen instead of not having PUT today, we&#39;d have PUT misimplementations\nrunning rampant.  Perhaps HTTP 1.1, instead of defining PUT as it is\nnow, would have been required to change to accommodate how PUT was being\nimplemented at the time?  Or maybe by not accommodating mis-use, HTTP\n1.1 would have failed in the marketplace, or more likely HTML 4 would\nhave failed to bring the browser wars to a close, possibly?\n\nIn the name of separation of concerns and the principle of generality,\nI&#39;d like to see modular HTML5, with an external forms module, and spin\noff Web Forms just as the XHTML 2 group spun off XForms.  Linking is\nfundamental to any hypertext language, forms not so much.  The reason\nis that links work cross-protocol, forms become protocol-dependent.\nAttaching @accept to links makes them protocol-dependent, and no longer\na copy-paste affair.\n\n(Hopefully, WHATWG got the memo about modular HTML languages, because\nthis makes it much easier to substitute one forms language for another,\nor adapt to future protocol changes, since only the forms and link\nmodules actually have anything to do with networking protocols, since\n&lt;meta http-equiv&gt; is deprecated, at least in XHTML 1.1 thankfully.)\n\n&gt; \n&gt; My initial suggestion was &quot;just a way to override the client&#39;s\n&gt; request headers without using JavaScript&quot;. I don&#39;t see how this can\n&gt; be achieved without adding this into markup, what&#39;s your alternative?\n&gt; \n\nWhile I agree that header manipulation should be possible in markup, I\ndon&#39;t think it belongs in a core language spec, but rather in a\nprotocol-specific extension, like XForms 1.1 is an HTTP-specific forms\nmodule which can work with HTML or even SVG.  Hopefully, HTML5 will\nallow authors to use XForms if they so desire, instead of restricting\ninnovation to what can be accomplished using name/value pairs and AJAX\nas HTML5 looks to be doing.\n\n&gt;\n&gt; I would turn that on it&#39;s head and ask; Why do you want to make so\n&gt; many, essentially, superfluous URI&#39;s? What is gained by doing this\n&gt; with a representation that is already addressable from this URI with\n&gt; the correct Accept headers?\n&gt; \n\nYou keep saying Accept headers, do you mean request headers?  Yes,\nREST development involves lots of URIs.  As Roy says, when designing a\nRESTful network-based application, the first thing you do is identify\nyour resources.  Not design your URI allocation scheme, identify which\nresources will be exposed as &#39;first-class objects&#39;.  Then you mint URIs\nfor them, i.e. put them in a hierarchy, or make them available by\nquery, etc. -- IOW, give them names (or Locations, i.e. URLs/URIs).\n\n(Then again, REST development limits the creation of URI aliases, I\nimagine a busy site using session URIs mints as many aliases in one day\nas a RESTful site mints URIs in a year.  Sites which avoid the creation\nof URIs by using session cookies defeat the purpose of caching, so what\non Earth is the benefit?  I&#39;d rather have my server&#39;s responses cache.)\n\n&gt; \n&gt; It&#39;s how content negotiation is *presently* done. I&#39;d always\n&gt; understood Resource as reference to any given abstract entity;\n&gt; &quot;report.html&quot; doesn&#39;t seem very abstract to me.\n&gt; \n\nThat&#39;s because &#39;report.html&#39; is a name, not a concept.  The concept,\nif I catch your drift, is &quot;report about something&quot; -- NOT &quot;report about\nsomething in HTML format&quot; or &quot;report about something in PDF format&quot;.\nI&#39;ve just identified three resources by minting URIs for them:\n\nhttp://example.org/report\nhttp://example.org/report.html\nhttp://example.org/report.pdf\n\nThe first URI has a 1:2 mapping of resource to representation with\nVary: Accept.  The other two URIs have a 1:1 mapping and no Vary\nheader.  All three are resources in their own right, the second two\nalso happen to be representations of the first resource.  The first URI\nis a negotiated resource, because multiple representations are\navailable.  Quoting REST:\n\n&quot;\nMore precisely, a resource R is a temporally varying membership\nfunction MR(t), which for time t maps to a set of entities, or values,\nwhich are equivalent. The values in the set may be resource\nrepresentations and/or resource identifiers.\n&quot;\n\nWith GZIP enabled on the server, the first URI now has a 1:4 mapping and\nthe other two now have a 1:2 mapping, and all three become negotiated\nresources.  The negotiated resource has (temporally speaking) gone from\nmapping to a set of one identifier and two representations, to mapping\nto a set of three identifiers and four representations.\n\nThe second two URIs are different resources because they are different\nconcepts: &quot;report about something in HTML format&quot; and &quot;report about\nsomething in PDF format&quot;.  The reason these two resources can share a\nthird URI, is because they represent the same application state\n(provided they&#39;re kept synchronized), i.e. they&#39;re both &quot;report about\nsomething&quot;, in fact they&#39;re the same report about the same something.\n\nOr, if you like, just have one resource:\n\nhttp://example.org/report\n\nAnd serve either HTML or PDF based on Accept header, and don&#39;t worry\nabout minting more URIs, or identifying new conceptualizations of the\nsame application state (report.atom).\n\nRegardless of which way you go, a negotiated resource won&#39;t be\ncacheable by HTTP 1.0 caches. HTTP 1.1 caches are different, but most\nwill refuse to cache a negotiated resource which fails to include a\nContent-Location header (with the exception of Vary: Accept-Encoding,\nonly, since compression is an either/or proposition).\n\nMinting three URIs is what makes it possible to use the Content-\nLocation header, which most caches require when encountering any opaque\nstring other than &quot;Vary: Accept-Encoding&quot; since there are an infinite\nnumber of possibilities beyond that either/or proposition.  When\nprovided with separate URIs for each representation, a cache has a much\neasier time narrowing infinite possibilities (tough to cache) down to a\nmanageable, separately-dereferenceable, separately-compressable set of\n(easy to cache) representations, with their own headers (like Etag).\n\nOne cache may dereference individual representations from the server,\nwhile the next cache uses Content-Location as an opaque identifier that\nis parsed by an algorithm, but not dereferenced.  What caches *do* with\nContent-Location varies, but I can&#39;t imagine a cache that doesn&#39;t need\nit to be there for content negotiation to work properly.  Which doesn&#39;t\nmean there aren&#39;t plenty of caches out there that don&#39;t work properly\neven with the Content-Location header...\n\n&gt;\n&gt; I would turn that on it&#39;s head and ask; Why do you want to make so\n&gt; many, essentially, superfluous URI&#39;s? What is gained by doing this\n&gt; with a representation that is already addressable from this URI with\n&gt; the correct Accept headers?\n&gt;\n\nThe reason REST identifies so many resources as first-class objects is\nfor visibility, a desirable property induced by applying the Uniform\nInterface constraints to an application... in fact, re-read Chapter 5\nof Roy&#39;s dissertation, then meditate on it for 45 minutes chanting the\nword &#39;visibility&#39; as your mantra.  Quoting REST:\n\n&quot;\nWithin REST, intermediary components can actively transform the content\nof messages because the messages are self-descriptive and their\nsemantics are visible to intermediaries...  REST enables intermediate\nprocessing by constraining messages to be self-descriptive: interaction\nis stateless between requests, standard methods and media types are\nused to indicate semantics and exchange information, and responses\nexplicitly indicate cacheability.\n&quot;\n\nStateless, self-descriptive, visible and therefore, more cacheable\nmessages are only possible, where Accept-header conneg is concerned, in\nthe presence of a Content-Location header.  Setting HTTP aside for a\nmoment, the whole point is that there needs to be a unique identifier\ninvolved somewhere (except in the either/or case of compression)for\neach representation, and that&#39;s the role URI fills in REST.\n\nSo, what&#39;s gained by these &#39;superfluous&#39; URIs is caching, and of\ncourse in REST there&#39;s no such thing as session state in the URI so\nthere isn&#39;t the cache failure involved with such approaches. If you\naren&#39;t attempting to cache negotiated resources, then you don&#39;t need to\nmint standalone URIs for each representation. However, the reduced\nvisibility which results from making the messages somewhat less than\nself-descriptive, which is exactly why they can&#39;t be cached.\n\nI&#39;d say that any inherently-uncacheable public response has broken the\nUniform Interface constraint.  I&#39;d also say that I mint exactly as many\nURIs as my application calls for -- no more, no less -- to be cacheable.\n\n&gt;\n&gt; &gt; Content negotiation has nothing to do with HTML, really.\n&gt; \n&gt; You&#39;re correct. That is the reason I have brought this up - to get\n&gt; give developers the opportunity to make browsers negotiate content\n&gt; better - without having to embed it in URIs.\n&gt; \n\nThe problem remains, not coupling markup (or URI) too closely with\nprotocol.  By far the most widely implemented form of content\nnegotiation is to compress response streams, and this does not involve\nmultiple URIs, Accept headers, or any markup.  In REST terminology,\ncontent negotiation is the mechanism used for the late binding of a\nrepresentation to the request URI.  I&#39;m not sure exactly what you mean\nby embedding conneg in URIs, but it sounds to me like the early binding\nof a representation to a resource, i.e. not conneg.\n\n&gt; \n&gt; I wouldn&#39;t store all of the request headers, only the ones that were\n&gt; not default.\n&gt; \n\nStill, I doubt browser vendors are going to go for the idea that a user\ncould type in the same URI they have bookmarked and see a different\nvariant than if they had used the bookmark for that same URI.\n\n&gt;\n&gt; &gt; HTTP headers are client specific, not request specific -- headers\n&gt; &gt; don&#39;t (shouldn&#39;t) change from one request to the next unless the\n&gt; &gt; client is upgraded. For example, an upgrade from Safari 2 to Safari\n&gt; &gt; 3 changes the Accept header to include &#39;application/xhtml+xml&#39;.  If\n&gt; &gt; a resource is negotiated based on Accept header, it may well give a\n&gt; &gt; different response to the upgraded browser -- it would be horrible\n&gt; &gt; practice if, when selecting a bookmark link, the browser sent the\n&gt; &gt; same stored Accept header it previously had, instead of the new one\n&gt; &gt; which more accurately reflects the client&#39;s capabilities, and\n&gt; &gt; letting the server do its conneg thing.\n&gt; &gt;   \n&gt; \n&gt; A browser is a special case UA though, I don&#39;t understand why it&#39;s \n&gt; Accept headers can&#39;t have their default overwritten from one request\n&gt; to the next. That makes perfect sense to me; the fact that it&#39;s not \n&gt; possible in practice is the reason I am bringing this up.\n&gt; \n\nI think you meant, not possible without JavaScript.  You haven&#39;t\nanswered the question about why a page author/generator can&#39;t link to\nthe URI returned in Content-Location instead of the negotiated URI.\n\n&gt;\n&gt; I see a bookmark as a store of a user&#39;s request, why would it be \n&gt; beneficial for an upgrade to cause a change in a user&#39;s bookmark?\n&gt; \n\nNo, a bookmark stores a location.  When I upgraded Safari from version\n2, to version 3 including XSLT, I expected sites that previously hadn&#39;t\nserved client-side XSLT to that browser now would.  Just as I once\nexpected when upgrading browsers to new versions supporting PNG instead\nof just GIF.  This is how the Web evolves.  Why on Earth would I want\nto have to upgrade all my bookmarks to support new features introduced\nby browser vendors, that involve changes to their request headers?\n\nThis would also lead to an expectation by users that the response to\nany request they make be locked in (tightly coupled), to receive the\nintended representation of a resource, if it&#39;s from a bookmark.\n\n&gt;\n&gt; &gt; That&#39;s tight coupling, not to mention needless complexity, and\n&gt; &gt; doesn&#39;t really solve any real-world problems, while precluding any\n&gt; &gt; benefit to using content negotiation in the first place, if all\n&gt; &gt; bookmarks must be altered when a browser is upgraded or stay locked\n&gt; &gt; in to a lower-quality representation.  The simple solution which\n&gt; &gt; has been proven to work, is to give each representation its own\n&gt; &gt; URI, instead of trying to invent uber-bookmarks for negotiated\n&gt; &gt; resources, or trying to implement content negotiation in markup.\n&gt; &gt;   \n&gt; \n&gt; I don&#39;t understand the tight coupling reference at all. Can you\n&gt; expand this at all?\n&gt; \n\nIn the K-meleon browser, let&#39;s say the next release supports GZIP, why\nwouldn&#39;t I want to start receiving compressed content from sites I&#39;ve\nbookmarked?  This idea of storing request headers would be complicated\nnot only to implement, but for users to comprehend -- heck, most Web\ndevelopers know nothing about request headers, why expose users to them?\n\nLoose coupling would be to store the only thing the user sees besides\nthe content -- its location.  This allows client, server and documents\nto evolve independently.  Request and response headers, like content,\nmust be allowed to evolve independently over time in order for a\nsystem to scale.  Trying to tie a stored URI to a once-upon-a-time\nrequest/response interaction is exactly like minting a new URI for\nevery revision of a document.\n\nThere&#39;s nothing to stop you from building a browser that works in such\nfashion and releasing it into the wild, just as there&#39;s now way to\nprevent authors/generators from versioning URIs.  But REST is the topic\nhere, and REST is an architectural style which emphasizes independent\ncomponents not just evolving over time, but scaling as well.  I think\nyou&#39;d have a problem, in terms of breaking REST&#39;s self-descriptive\nmessage constraint, which would lead to caching issues.\n\nIf a user is now expecting a server to always return the historical\nresponse to the stored historical request, the resulting interaction\nbecomes opaque to intermediaries, i.e. not self-descriptive -- instead\nof describing the current client-server interaction, the attempt seems\nto be to describe a static interaction, and I&#39;m still not seeing why\nit&#39;s necessary in the first place.\n\nAgain, since the value of Content-Location is a URI which may be\nbookmarked (in the REST sense of the term), why can&#39;t that URI be\nbookmarked?  You could also build a browser that does this, or maybe\nsome client(s) out there already do, in that when a Vary header is\nreceived the client bookmarks Content-Location instead of the\nnegotiated URI.  Same problem to my mind, though.  Either side of the\ncontent negotiation equation could change the next day, and render the\nstored Content-Location obsolete in response to any headers sent, which\nis why loose coupling is best -- it&#39;s why applications implement conneg.\n\n&gt; \n&gt; &gt; In REST, any resource of interest is given a URI, such that\n&gt; &gt; application state may be bookmarked.  But, that application state\n&gt; &gt; is not a function of the Content-Type.  A compressed representation\n&gt; &gt; doesn&#39;t need its own URI.  The argument for using separate URIs for\n&gt; &gt; image.jpg/image.png has more to do with proper caching, than being\n&gt; &gt; able to bookmark them separately, since they represent the same\n&gt; &gt; application state. \n&gt; \n&gt; So serving multiple representations from one URI doesn&#39;t work with\n&gt; caching?\n&gt; \n\nNot without a Content-Location header, no.  What else does a cache have\nto go by?  Let&#39;s say Etag for the sake of argument.  A cache which\nreceives a response with Vary, Cache-Control and Etag headers but _not_\nContent-Location may not bother trying to cache it at all (the more\nstuff in Vary the fewer caches it&#39;s compatible with, is a good rule-of-\nthumb to follow).  Without Content-Location, or with Content-Location\nbut not Vary, the cache behavior I&#39;ve observed in those caches which\ndon&#39;t just ignore, is as follows:\n\nThe client makes a request to the negotiated URI /image .  The server\nresponds with image.jpg and an Etag.  The cache stores image.jpg for\nthat URI.  The next client requests /image and the server responds with\nimage.png and a new Etag, so the cache sees the new Etag and expires\nimage.jpg in favor of image.png for /image.  The next client requests\n/image and the server responds with image.png and its Etag, so the\nclient hits the cache.  The next client requests /image and the\nresponse is image.jpg, so the cache expires image.png and replaces it\nwith image.jpg, and so on and so forth.  Representations take turns\ncaching.\n\nWith Content-Location (don&#39;t forget Vary), caches have that other vector\nnecessary to determine expiration, besides Etag or other cache-control\nheaders.  Now a cache can correlate an Etag not to the negotiated\nrequest URI, but to the representation-specific URI, so image.png and\nimage.jpg will work properly with their existing Etags, or any future\nEtags, without impacting the cachability of negotiated responses.\n\nOnce again, the exception is Accept-Encoding, since any cache knows\nwhat to do with that.  I tried an experiment where I based content\nnegotiation on Accept-Encoding for a purpose besides compression, it\ndid not work -- caches flipped between one or the other representation\nas above, compressed or raw depending, though.  So K-meleon could prime\nthe cache with the raw representation, then the cache would compress\nthat and serve it to Firefox and Opera, so Accept-Encoding negotiation\ncan only be used for compression.  But, it works amazingly well across\nthe board for that, the exception being IE &lt; 7 on systems without .NET.\n\n&gt;\n&gt; If I we use the URI for conneg..\n&gt; \n&gt; and I PUT changes to /report.doc - how does my cache know that it\n&gt; needs to refresh /report.pdf and /report.xml ?\n&gt;\n\nThat&#39;s a good question, Mike.  But it bears a sub-question around\nhere:  by &quot;my cache&quot; do you mean cache component, or cache connector?\nIf connector, client or server?  ;-)\n\nThe answer to your question depends on how you&#39;re generating\nyour documents.  Let&#39;s say, for the sake of argument, that you&#39;re\ntransforming report.pdf from report.xml using XSL-FO and a PDF\nutility, and transforming report.doc and report.html from report.xml\nusing XSLT. If you PUT a change, the best place to PUT it would be\nreport.xml, so it can be validated for well-formedness and/or checked\nagainst a schema before being fed into any XSL transformation pipeline.\n\n(Or, make it report.atom and use Atom Protocol to interact with it.)\n\nWhen your application accepts the PUT, it makes a GET request against\nthe other variants.  This causes them to regenerate from the updated\nreport.xml, and they&#39;re assigned new Etags.  If you&#39;re using a cache\nconnector on your origin-server component, make sure to include Cache-\nControl: max-age=0 as a header when you GET the other variants, to\nindicate that you want the origin-server&#39;s response, not the cache&#39;s.\n\nSo you don&#39;t accept PUT on the negotiated URI at all.  Since hypermedia\nis the engine of application state, your user interface code instructs\nthe User-Agent to PUT to report.xml.  Or, make the PUT to /report and\nonly Accept: application/xml at that URI.  I like the former, what if\nreport.xml is also the response for http://example.org/annual-report ?\nWould you accept PUT at /annual-report also?  I like working directly\nwith the resource I&#39;m altering.\n\n-Eric\n\n"}}