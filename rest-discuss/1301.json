{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":23186829,"authorName":"Paul Prescod","from":"Paul Prescod &lt;paul@...&gt;","profile":"papresco","replyTo":"SENDER","senderId":"sOYqn0DxvATTq7cOxnfDGL-NvqmkOTRes-301c8ZTmmIiYT_92_ths8hdWZh_iOn9sHMCc6j25x0Ut6Uvzibovl4BkmnWA","spamInfo":{"isSpam":false,"reason":"0"},"subject":"REST/SOAP ideas summary","postDate":"1021961785","msgId":1301,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDNDRTlFNjM5LkRBRjQyNzVCQHByZXNjb2QubmV0Pg=="},"prevInTopic":0,"nextInTopic":1303,"prevInTime":1300,"nextInTime":1302,"topicId":1301,"numMessagesInTopic":3,"msgSnippet":"(very rough) == Roots of the REST/SOAP debate == There is one thing we can all agree upon: the REST vs. SOAP debate is complicated.  Debates about architecture","rawEmail":"Return-Path: &lt;paul@...&gt;\r\nX-Sender: paul@...\r\nX-Apparently-To: rest-discuss@yahoogroups.com\r\nReceived: (EGP: mail-8_0_3_2); 21 May 2002 06:15:47 -0000\r\nReceived: (qmail 48594 invoked from network); 21 May 2002 06:15:47 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m5.grp.scd.yahoo.com with QMQP; 21 May 2002 06:15:47 -0000\r\nReceived: from unknown (HELO smtp1.ActiveState.com) (209.17.183.249)\n  by mta1.grp.scd.yahoo.com with SMTP; 21 May 2002 06:15:46 -0000\r\nReceived: from smtp3.ActiveState.com (smtp3.ActiveState.com [192.168.3.19])\n\tby smtp1.ActiveState.com (8.11.6/8.11.6) with ESMTP id g4L6Fcb31561\n\tfor &lt;rest-discuss@yahoogroups.com&gt;; Mon, 20 May 2002 23:15:38 -0700\r\nReceived: from prescod.net (ssh1.ActiveState.com [192.168.3.32])\n\tby smtp3.ActiveState.com (8.11.6/8.11.6) with ESMTP id g4L6Fae24940\n\tfor &lt;rest-discuss@yahoogroups.com&gt;; Mon, 20 May 2002 23:15:36 -0700\r\nMessage-ID: &lt;3CE9E639.DAF4275B@...&gt;\r\nDate: Mon, 20 May 2002 23:16:25 -0700\r\nX-Mailer: Mozilla 4.76 [en] (Windows NT 5.0; U)\r\nX-Accept-Language: en\r\nMIME-Version: 1.0\r\nTo: &quot;rest-discuss@yahoogroups.com&quot; &lt;rest-discuss@yahoogroups.com&gt;\r\nSubject: REST/SOAP ideas summary\r\nContent-Type: text/plain; charset=us-ascii\r\nContent-Transfer-Encoding: 7bit\r\nX-Perlmx-Spam: Gauge=, Probability=0%, Report=&quot;INTERNAL_HOST&quot;\r\nX-Filtered-By: PerlMx makes it fast and easy.  See http://www.ActiveState.com/Products/PerlMx/Header\r\nFrom: Paul Prescod &lt;paul@...&gt;\r\nX-Yahoo-Group-Post: member; u=23186829\r\nX-Yahoo-Profile: papresco\r\n\r\n(very rough)\n\t== Roots of the REST/SOAP debate ==\n\nThere is one thing we can all agree upon: the REST vs. SOAP debate is\ncomplicated.  Debates about architecture are invariably complicated\nbecause they inevitably revolve around abstract models as opposed\nto syntactic details. In many cases different people&#39;s mental models\nhave developed differently through their years of experience.\n\nThis paper is intended to help readers to understand the point of\nview of a REST advocate at a high level.\n\nREST stands for REpresentational State Transfer, which is Roy\nFielding&#39;s name for the architecture of the current Web. If the Web\nhad had a design document in advance, it would have outlined the\nprinciples of REST. But The Web did not have a design document and\nso many people think it merely accumulated. Nevertheless, the design\nof the Web can be determined by analyzing the major documents that\ndescribe its protocols and formats: URIs (RFC XXXX), HTTP (RFC XXXX),\nHTML (RFC XXXX) and XML (RFC XXXX).\n\nThe term REST has evolved (for better or worse) from what Roy\nFielding used it to mean. He was mostly focused on discussing the\nvirtues and vices of HTTP. What most people mean when they say that\nthey support REST is that they want Web services to integrate deeply\ninto the architecture of the Web and use Web technologies to their\nfullest. REST only became a rallying cry when it became clear that\nan alternative model was arising and that this alternative was in\nsome senses competitive with the Web.\n\n\t== In the Beginning ==\n\n\nThere are four main concepts that serve as the characters in this\nstory. You are probably familiar with them but I need very precise\ndefinitions so bear with me.  Our protagonists are documents, data,\nprograms and protocols. Historically a document was a bag of bits\nintended for viewing by a human being. Documents were written by\npeople for people.  Because people are creative, documents vary\nradically in size and shape. Data is more often &quot;collected&quot; rather\nthan &quot;created&quot;. For easy maintenance it is often collected in very\nconstrained ways and stored in highly organized containers: databases.\nPrograms are sets of instructions written in some language, usually a\nprogramming language.  Protocols are ways for programs to communicate\nwith each other across computer networks.\n\nWithout programs there would be no computing so in that sense programs\nare wonderful. But in another sense they are a necessary evil.\nIn particular, programs are extremely hard to analyze. That is why\nthere are so many bugs in them! Computers can do only very rough\nvalidity checks on them.  Programs are also quite hard to use in new\ncontexts that they were not prepared for.  A Macintosh program cannot\n(in general) run on a PC. This is because it is hard to analyze the\nprogram and understand what parts of it depend on what quirks of\nthe Macintosh platform. If programmers need to write code that runs\non both Macintoshes and PCs, they tend to code in a very particular\nway from the beginning (choosing their tools carefully and testing as\nthey go). In essence they do the required analysis a little bit at a\ntime as they go rather than expecting to be able to automate it later.\n\nTo put it another way: if somebody drops 100MB of data into your lap\nand asks you to do something useful with it, there are a variety of\nanalysis tools and techniques you can use to determine its nature and\nfigure out how to re-use it. If they drop the same 100MB of program\ncode into your lap you will find that the analysis is in general much\nmore difficult because analyszing code is intrinsically difficult.\n\nAround the late 1960s, some IBM researchers noticed that existing\ntechnologies did not make a clear boundary between programs\nand documents. Some forms of documents were really just simple\nprograms. Some were actually very complex programs! One virtue\nof modern computers is that they can treat code as data and data\nas code but usually it makes more sense to treat them as separate\nentities. The researchers noticed that if you treat documents as data,\nrather than programs, you can much more easily repurpose them for\nnew environments. They sought to clearly delineate the line between\ndocument data and the code that processed it.\n\nLet me give a concrete example. Suppose you work in a highly secure\nenvironment.  Someone sends you a Word document and a Docbook/XML\nversion. Word mixes code and data. Docbook/XML does not. You load the\none up into Word and the other up into an XML editor. There is a chance\nthat the first one will cause Word to say something along the lines\nof: &quot;I&#39;m sorry. There seems to be a macro in this program. Macros are\nprogram code. Analyzing program code is extremely difficult. Therefore\nI can&#39;t tell you whether this document is safe or unsafe. Open it at\nyour own risk.&quot; The Docbook version will never give such a warning\nbecause Docbook separates document data from code so that there is\nno need for such a security audit. Similarly, Word macros tend not\nto work properly in Word-like programs that claim to work with Word\ndocuments. Docbook documents are entirely platform independent.\n\n\n\t== SGML Philosophy ==\n\n\nDocbook was orginally based upon SGML. SGML was the language that came\nout of that research at IBM. Docbook&#39;s easy environment-independence\ncomes from using the two markup languages and keeping in mind the\nphilosophies that surround them.\n\n\nPeople who grew up with SGML and XML came to internalize certain\nphilosophies. One was that separation of data and processing is\nsometimes inconvenient in the short term but generally pays dividends\nin the long term. Once you&#39;ve separated the data from its processing\nyou can often find new uses for the data that you did not originally\nexpect. Because it is easy to analyze and reuse data, not programs,\nSGML people tend to strongly prefer systems that encode as much of\nthe logic as possible in data rather than programs.\n\nAverage programmers, not surprisingly, feel uncomfortable with this.\nIf programming is what you get paid for, why would you want to\ndecrease the importance of programs? Moving logic from program code\nto data is also a hard sell for the same reason that jogging is a\nhard sell. It often (but not always) involves short-term pain for\nlong-term gain. Furthermore, the nature of the long-term gain is\ntypically unforseeable. Any well-defined, present requirement can be\ncoded into a program. It is the ill-defined, future requirement that\nthe program will probably have problems with. Because data is easier\nto analyze, it is easier to repurpose it to solve new problems easier.\n\nMost people do not understand that the Web was designed precisely\nin opposition to the idea that data should be accessed through\nprogrammatic methods rather than being treated as intrinsically \ninert.\n\nhttp://www.w3.org/DesignIssues/Principles.html#PLP\n\nThe most advanced techniques for moving logic from code to data are\ncalled &quot;knowledge technologies.&quot; RDF is the knowledge technology from\nthe W3C. Topic maps are from ISO. These knowledge technologies allow\nthe classification of information items, the association of metadata\nwith the items and allow relationships betwen items to be expressed\nexplicitly in some cases and inferenced (inferred) in other cases.\nInferencing is a way of uniting partially understood information\nsources to create new knowledge.\n\nPeople vary in their level of skepticism about the applicability\nof these knowledge technologies.  In my mind, that debate is\nmarginal. Sophisticated developers separate data from processing as\nmuch as possible, whether or not the data conforms to some particular\nstandard. The knowledge technology standards are just points far\nalong a spectrum of declarative technologies. Every hyperlink in the\nworld is a declarative assertion of a relationship between two data\nobjects. If you understand the benefit of hyperlinks then you \nunderstand what declarative techniques can do for you.\n\n\t== The SGML Tribe ==\n\nMembers of the SGML tribe also came to deeply revere the related\nconcepts of linking and addressing. Once again, this comes back to\nthe idea of reusing information in ways that it was not originally\nintended for. If you can get some kind of handle or reference to\nan information item, you can reuse it in new forms of information\npackaging. For instance, it is possible to link a range of Excel cells\ninto a Word document. As you change the cells in Excel, the changes\nare reflected in Word. You don&#39;t have to create the original Excel\nspreadsheet with some special flag that says: &quot;I want to reuse this\ninformation.&quot; You just tell Word (through drag and drop) that you\nare interested in cells A3-D5 in &quot;myfile.xls&quot;.\n\n\nSGML people took this technique to new heights and created extremely\nsophisticated systems built upon it. The most sophisticated of these\nsystems is the World Wide Web, built in part by people who were\npart of the SGML tribe (like Dave Raggett), in part by people who\nhad discovered or reinvented its core ideas without joining the tribe\n(like Tim Berners-Lee) and also by people who simply did not understand\nit at all (like Marc Andreesen and is cronies, who I will probably\nnever forgive for lamentably poor extensions to HTML and the Web).\n\n\n\t== The Google Example ==\n\nLet us consider a particular example. One way to do hyperlinking is\nwith declarative statements about what pages are linked to other\npages. Another way is to put little programs in each element that\ninvoke the hyperlinking behaviour.\n\nA programmatic approach to hyperlinking would encourage you to\nrepresent the relationships between pages as little snippets of code\nthat load and display the next page.  At first glance this would seem\nto be a more sophisticated system because it would be possible to write\nsophisticated hyperlinks that do complicated computations and then\nchoose the appropriate page to load based on those computations. The\ncode could also do interesting things with the placement and styling\nof the pages.\n\n\nhttp://www.w3.org/DesignIssues/Evolution.html#Least\n\nUnfortunately, code is difficult to analyze. This means that once\ncode is deployed the user typically has only a boolean choice:\n&quot;run it&quot; or &quot;don&#39;t run it.&quot;\n\nOn the other hand, consider one result of our declarative approach\nto hyperlinking: Google. Google is built upon the analysis of Web\nhyperlinks. If hyperlinks were code rather than data, Google could\nnot come along ten years after the invention of the Web and figure\nout a completely new way for analyzing and reusing the deata embedded\nin it. Nobody foresaw Google&#39;s ranking algorithm when the Web was\ninvented. In fact the idea of a central search engine would probably\nhave seemed insane. But years later, the people at Google noticed\nall of the Web data sitting around and they found a new way to make\nit valuable. Yahoo has a similar story.\n\nAnother interesting thing about Google is that it works despite the\nfact that it does not always know exactly what it is doing. Google\ndoes not know when it sees a link of whether it is a link from a\nchild document to a parent document or vice versal.  It does not\nknow whether the two documents are created by the same person. Google\ndoes not know why the documents are linked. Google sees a link and it\nrecords it. It works from partial understanding rather than waiting\naround for complete information.\n\nIf Google could only get information out of a service by understanding\nevery detail about the service&#39;s interface, it would not be able to work\nwith partial understanding and will be consequently crippled (later\non I present an analogy with the phone system that makes this clearer).\n\nhttp://www.w3.org/DesignIssues/Evolution.html#PartialUnderstanding\n\nWeb-based services such as Google, Yahoo, Blogger and Meerkat add value\nto existing information by making links to that information. They\nshow what you can do by using XML and hyperlinks to create services\nthat work with partial information based on analysis of data.\n\n\n\t== SOAP&#39;s Role ==\n\nThe programming world spent years trying to avoid using a data-centric\nmodel for networking. They used protocols such as DCOM and CORBA which\nhid the data behind programmatic interfaces (APIs) and delivered the\ndata across the network in unreadable binary packets. These are called\nRemote Procedure Call (RPC) protocols. One camp of people thought\nthat there were only two things wrong with this strategy. First,\nthey disliked that there were BOTH DCOM and CORBA. They logically\nfelt that there should really be a single standard for RPC over the\nInternet. Also, they thought that the binary packets were turning\noff Internet programmers, who are more comfortable with text-based\nformats. This group invented what we will call &quot;SOAP-RPC&quot;, which was\nan XML-based remote procedure call protocol for use over the Web.\n\nAnother camp thought that this was not enough to allow messages to flow\nfreely between businesses. They thought that an important requirement\nwas &quot;loose coupling&quot;. In other words they wanted to make it possible\nfor clients and servers to evolve independently. XML allows this but\ndoes not require it. It takes extra effort to create systems that are\nloosely coupled. Over the last several years, people espousing this\nview have taken over the job of directing the SOAP specification. Most\nof the more clued-in architects at vendor companies agree. They see\nthe RPC model as backwards.\n\nhttp://www.prescod.net/soap/views\n\nNeither camp incorporated lessons from the SGML and XML tradition. In\nparticular they did not design SOAP such that individual data objects\nwould be addressable. For instance, if you were designing a banking\nweb service using standard SOAP tools, you would almost certainly\nmake the bank the only explicitly addressable object. In other words\nthe only URI would be for the bank. To send a message to a particular\naccount you would do something like this:\n\nbank = new SOAPProxy(&quot;http://.....&quot;)  bank.addMoneyToAccount(account\n23423532, 50 dollars)\n\nTo get information about a particular user, you would do something\nlike this:\n\nbank = new SOAPProxy(&quot;http://.....&quot;)\nbank.getUserNameFromAccount(account 23212343)\n\nNote that the account itself is not addressable. It has no URI.  In the\nWeb-centric version of the service the accounts would be addressable.\n\nLet me offer an analogy. Suppose you were living temporarily in\na hotel.  The hotel might not have direct dial connections from the\noutside. In order to call a room you have to contact the operator first\n(this is like contacting the &quot;SOAP endpoint&quot;) and then ask them to\nconnect you to your room. Now imagine that there is an outside service\nthat you would like to buy. It is a onc-a-day automated wake-up call\nand horoscope service. You try to sign up for the service but when\nyou are asked to enter the phone number to call back you realize that\nthere is no single number. The service must contact the operator first\nand then the operator must patch them through to you.  Obviously the\ncomputer on the other end is not going to be smart enough to know to\ngo through the operator and the operator will not know to patch the\ncall through to your room.\n\nIf everybody lived in a hotel like that, the operator service would\nbe practically impossible. A particular application of the phone\nsysstem would simply cease to exist. Telemarketers would find their\njob a little harder too but I think that they would adapt! \n\nNote that the problem is not obvious in the design of either system. It\nis when you try to unite the two systems that you wish that the hotel\nhad used the international standard phone addressing &quot;syntax&quot; rather\nthan having an extra level of misdirection through the operator. SOAP\nservices are the operator. The objects they work with (purchase orders,\nbank accounts, personelle records) are the hotel rooms. The data is at\nthe mercy of the soap endpoint. If the interfaces of the client and\nserver do not exactly align, the two cannot communicate. And yet,\nparticipants are often happy to work with partial knowledge and\nloosely coupled interfaces. For instance, the automated horoscope\nsystem was not interested in the fact that you happened to be in a\nhotel room. Similarly, a system for tabulating monthly pay checks would\nnot care what precise human resources management system the enterprise\nwas using. As long as the employee records are available in EmployeeML,\nit doesn&#39;t matter what workflow was used to get them there.\n\nBear in mind that SOAP&#39;s weakness around addressing does not stem\nmerely from ignoring the lessons of XML and SGML. In fact, older\nstandards like CORBA and DCOM did much better in this area. Every\nobject had an address and although the address syntaxes were not\nURIs, they were at least standardized within the domain of each RPC\nprotocol. SOAP lacks any equivalent addressing model.  Although it\nis the new, new thing, it is actually less sophisticated in this way\nthan its predecessors.\n\n\t== SOAP and the Web ==\n\nThere is a reason for SOAP&#39;s weakness around addressing. If SOAP\nunambiguously stated that the addressing syntax for SOAP is URIs then\nthat would be equivalent to saying that SOAP is designed for use on\nthe Web and only on the Web. But SOAP advocates are quite clear about\nthe fact that Web protocols and Web addressing models serve only as\n&quot;transports&quot; for SOAP. That is like saying that the Web is a taxi\ndriver and its only job is to get SOAP from place to place. The SOAP\nmessage can get out of the taxi and into another vehicle for the next\nleg of its trip. In technical terms, SOAP &quot;tunnels&quot; through the Web.\n\nThis is a core area of digression between the REST viewpoint and\nthat of the SOAP specificatoin. The REST viewpoint is that the \nWeb architecture is incredibly scalable and well-designed. The Web \nwas explicitly designed\nto integrate disparate information systems. But the Web did it on its\nterms, by binding them all into a single namespace and encouraging them\nto use a single protocol. The central virtue of the Web is that you\ndon&#39;t glue system X and system Y together using the Web as transport\nmiddleware. Rather you make a web interface to system X and a web\ninterface to system Y and the two systems can link to each other and\nexchange information with each other just by virtue of the fact that\nthey are using the same namespaces, protocols and formats.\n\nYes, the Web is middleware. But like any middleware you would buy from\na high priced vendor, the middleware encourages you to map your data\ninto a common model. This model is REST, the Web Architecture. Any\nmiddleware vendor will tell you that you want to do this kind\nof mapping rather than create N*M connections between individual\nservices. Even so, some people really want to take the short-cut of\ntunnelling through the Web. In some cases that might be useful but it\nis highly debatable whether it is the W3C&#39;s job to make that easier\nrather than concentrating on improving the Web so that tunnelling\nis not necessary. I, for one, am in favour of treating the Web as a\ntranslator rather than a taxi driver.\n\n\n\t== Interoperability is Key ==\n\nMany of the usage scenarios for SOAP seem to be inddirect\npoint-to-point integrations of system X and system Y, using the\nWeb as transport middleware. Considering how many systems there are\nout there, this model cannot scale. It quickly becomes obvious that\nthere will need to be standardized interfaces with many interoperable\nimplementations, just as there are many interoperable implementations\nof HTTP and SMTP. SOAP itself does not guarantee interoperability. It\nis protocols built on top of SOAP (probably specified in WSDL) that\nwill guarantee interoperability.\n\nThese standardized interfaces will be the basis of the Web Services\nrevolution. Some vendors promote SOAP as a way that any half-decent\nprogrammer can generate a new protocol by running a tool over their\nJava or C# program. Even the vendors are coming to agree that this\nis a naive point of view. First, it is naive because the services so\ngenerated are extremely poor from a distributed computing point of\nview. They are as brittle as communion wafers. They are as tightly\ncoupled as salsa dancers. Try adding an extra parameter on the\nGoogle service!\n\nSecond, it is naive because a blossoming of thousands of protocols\ndoes not get us any closer to interoperability. What we need for\ninteroperability are a few well-engineered, well-designed, scalable,\nsecure protocols. The success of the existing web and of email shows\nthat a few protocols can get a ton of work done. When you book a\nplane trip through an online travel agent, that agent isn&#39;t using the\n&quot;Travel Agent Protocol.&quot; It uses the generic HTTP protocol.\n\n\nHTTP is a special protocol in that it explicitly embodies the Web&#39;s\nprinciples. In particular HTTP revolves around URIs and uses very\nself-describing messages. A radical viewpoint is that HTTP is the\nonly web services protocol we will need!\n\nWhether or not this is true, there is a sense that the whole SOAP\nexercise has been a distraction from the main event. The main event\nis determining the data models and data flows necessary to allow\nebusiness to begin to flow.  ebXML (whether it turns out to be a\nsuccess or failure) is nevertheless attempting to solve the right\nproblems. The SOAP prject is not.\n\nBut SOAP is not just a distraction. It actually works against the\nprocess of getting ebusiness correct by training application developers\nto avoid using one of their most powerful tools, the hyperlink. This\nis analogous to teaching database designers not to use foreign keys,\nC programmers not to use pointers or Java programmers not to use\nreferences.\n\n\t== Separation of Processing from Data ==\n\nRemember that one of the lessons from the SGML days was that it is\ngood to separate data and program code? The HTTP protocol is brilliant\nat ensuring that data is not dependent on code. For instance imagine\nthat the department of motor vehicles had a web service that could\ndeliver information on who owns a particular license plate. I could\nintegrate the information into an XML document using one line of code:\n\n&lt;xi:include href=&quot;http://..../license_handler?V6A4G5&quot;&gt;\n\nThis will execute an HTTP GET. The important thing is that HTTP defines\na protocol for turning URIs into bit streams that can be incorporated\ninto other documents.\n\n\nThe web infrastructure guarantees that a client may execute this as\noften as they like with no repurcsions because GET operations are\nguaranteed to be safe.\n\nOn the other hand, SOAP would require me to somehow embed a method\ncall in my XML document. There is no syntax for doing this and there\ncannot be one. One of the Web&#39;s principle axioms is that users are not\nresponsible for any damage that is done by following a hyperlink. SOAP\nmethods have no way to communicate whether the method does or does\nnot do anything that could be considered damaging. This shifts the\nresponsibility to the client software in a manner that is not scalable.\n\n\n\t== REST From a Protocols Point of View ==\n\nThere is another way to come to the REST position without any interest\nin Web architecture. One can look at SOAP purely as a protocol without\neven considering the problems it is supposed to solve. It bends or\nbreaks all of the rules common among Internet protocols. For instance,\nif one reads a book on the roles of networking protocols, it is clear\nthat SOAP is a layer 6 (presentation layer) protocol but SOAP actually\nruns on top of HTTP and SMTP, which are both layer 7 protocols. That\nmakes SOAP a layer 8 protocol. Unfortunately the standard network stack\ndiagrams max out at 7! By definition the 7th layer, the application\nlayer, is the top layer. SOAP treats layer 7 protocols as &quot;transport&quot;\nprotocols. It pretends that they are layer 4 protocols. This causes\nvarious sorts of discomfort for system administrators, security\nanalysts and protocol purists.\n\nThere is a sense that there will be one or two widely publicized SOAP\nsecurity breaches and firewall administrators will shut SOAP traffic\ndown. Although any protocol can be used as the basis for a security\nhole, SOAP arguably encourages security holes by encouraging every\nbusiness analyst to become a protocol designer.  Also, the industry\nhas not been clear from a marketing perspective that SOAP is just a\nbasis for application protocols, not a top-level protocol itself. That\nmeans that SOAP could get tarred with the security flaws that appear\nin protocols based upon it.\n\nBut REST&#39;s biggest hole when it comes to protocols is the issue\nI discussed of many protocols versus a few. No &quot;many protocols&quot;\nframework has ever taken hold on the Internet. Security concerns\nare one reason. It is easier to analyze and understand the security\nimplications of a few standarized protocols rather than hundreds of\nunstandardized ones. Another reason is administration.\tAdministrating\na firewall that supports many protocols is harder than maintaining\nfew. Finally there is the big one, interoperability. Getting all of\nthese independently created protocols to talk to each other will be\na nightmare.\n\n\n\t== HTTP Does More than you Think ==\n\nHTTP is wonderful in that it natively separates data from\nprocessing. But it is also more general than just a document\nfetching protocol. Most people do not know that HTTP is 100%\nCRUD-compliant. That means that it has methods that map to &quot;create&quot;,\n&quot;retrieve&quot;, &quot;update&quot;, &quot;delete&quot; and &quot;replace&quot;, just as SQL does. The\npower of URIs and of the HTTP methods combine to make HTTP an extremely\ngeneral protocol for manipulating information sources.\tThe SQL\nanalogy should be suggestive of the sort of flexibility available!\n\n\n\t== Limits of The Web-As-We-Know-It ==\n\nHTTP is not perfect. The Web is not perfect. We are not at the end\nof history. We need new and better stuff. The REST argument is that\nwe need to understand what we have and make certain we do not lose\nfeatures we have today. SOAP makes it extremely difficult to use\nhyperlinks because SOAP is fundamentally a technology for tunnelling\nthrough the Web.\n\n"}}