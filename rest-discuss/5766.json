{"ygPerms":{"resourceCapabilityList":[{"resourceType":"GROUP","capabilities":[{"name":"READ"},{"name":"JOIN"}]},{"resourceType":"PHOTO","capabilities":[]},{"resourceType":"FILE","capabilities":[]},{"resourceType":"MEMBER","capabilities":[]},{"resourceType":"LINK","capabilities":[]},{"resourceType":"CALENDAR","capabilities":[]},{"resourceType":"DATABASE","capabilities":[]},{"resourceType":"POLL","capabilities":[]},{"resourceType":"MESSAGE","capabilities":[{"name":"READ"}]},{"resourceType":"PENDING_MESSAGE","capabilities":[]},{"resourceType":"ATTACHMENTS","capabilities":[{"name":"READ"}]},{"resourceType":"PHOTOMATIC_ALBUMS","capabilities":[]},{"resourceType":"MEMBERSHIP_TYPE","capabilities":[]},{"resourceType":"POST","capabilities":[{"name":"READ"}]},{"resourceType":"PIN","capabilities":[]}],"groupUrl":"groups.yahoo.com","intlCode":"us"},"comscore":"pageview_candidate","ygData":{"userId":3702469,"authorName":"Nic","from":"Nic &lt;nferrier@...&gt;","profile":"nferrier_tapsellferrier","replyTo":"SENDER","senderId":"e9mEm6-PLLrNsjEr4XWilEQ_gPKsUoUo97fKDFSmSwzGP9nozCQ14mSZ57n56n5S0fRUTpxvi7pM4Bl432Q9N2MPXOWhorIOyEM","spamInfo":{"isSpam":false,"reason":"12"},"subject":"Re: [rest-discuss] content negotiation, single resources, and the web today","postDate":"1143171276","msgId":5766,"canDelete":false,"contentTrasformed":false,"systemMessage":false,"headers":{"messageIdInHeader":"PDg3ZnlsOGxkbWIuZnNmQHRhcHNlbGxmZXJyaWVyLmNvLnVrPg==","inReplyToHeader":"PEQ0MkFEMzY5LTU3MjQtNDk0Ny04MDZFLTY2NjQ1NDZBNzkxQkB3My5vcmc+IChLYXJsIER1Ym9zdCdzIG1lc3NhZ2Ugb2YgIkZyaSwgMjQgTWFyIDIwMDYgMTE6MzY6MzAgKzA5MDAiKQ==","referencesHeader":"PDQ0MTg2RERELjYwMTAzMDVAam9ubmF5Lm5ldD4gPDQ0MjE5NEU5LjgwMDA2MDlAdmVpY29uLmNvbT4JPDg3YWNiaW1ncTIuZnNmQHRhcHNlbGxmZXJyaWVyLmNvLnVrPiA8NDQyMUE5RjguNTA3MDUwM0B2ZWljb24uY29tPgk8NDQyMUY1MzAuODAwMDgwMkBzZWFpcnRoLmNvbT4gPDg3NHExcWx6NGMuZnNmQHRhcHNlbGxmZXJyaWVyLmNvLnVrPgk8NDQyMjMzNkMuNTAwMDMwN0BzZWFpcnRoLmNvbT4gPDg3eTd6MWw2Y28uZnNmQHRhcHNlbGxmZXJyaWVyLmNvLnVrPgk8NzNlYzU5OWQwNjAzMjMwNjA4cDJhYmU1MTRjdWNiNDUxZGNkMjQ0NjcxZDhAbWFpbC5nbWFpbC5jb20+CTw4N3ZldTVreTB4LmZzZkB0YXBzZWxsZmVycmllci5jby51az4JPDczZWM1OTlkMDYwMzIzMTM0M3M0MjNjYmM5Mm80NjhmMGNlZjZlN2UwNmE4QG1haWwuZ21haWwuY29tPgk8ODdwc2tjbHQ5MS5mc2ZAdGFwc2VsbGZlcnJpZXIuY28udWs+CTw4MERGMkZFQi1ENUY1LTQyM0YtQTc5RS00RjY0NDIyRjUxOThAdzMub3JnPgk8ODdsa3YwbGhhMi5mc2ZAdGFwc2VsbGZlcnJpZXIuY28udWs+CTxENDJBRDM2OS01NzI0LTQ5NDctODA2RS02NjY0NTQ2QTc5MUJAdzMub3JnPg=="},"prevInTopic":5765,"nextInTopic":5767,"prevInTime":5765,"nextInTime":5767,"topicId":5693,"numMessagesInTopic":89,"msgSnippet":"... Yes. As you said there s no way of fixing it using Well Known URIs. ... eh? Why would the user retrieve the robots file? And actually pretty much the first","rawEmail":"Return-Path: &lt;nferrier@...&gt;\r\nX-Sender: nferrier@...\r\nX-Apparently-To: rest-discuss@yahoogroups.com\r\nReceived: (qmail 44856 invoked from network); 24 Mar 2006 03:53:14 -0000\r\nReceived: from unknown (66.218.67.36)\n  by m28.grp.scd.yahoo.com with QMQP; 24 Mar 2006 03:53:14 -0000\r\nReceived: from unknown (HELO owls-house.tapsellferrier.co.uk) (80.168.156.78)\n  by mta10.grp.scd.yahoo.com with SMTP; 24 Mar 2006 03:53:14 -0000\r\nReceived: from [172.31.100.241] (helo=wikidemo)\n\tby owls-house.tapsellferrier.co.uk with esmtp (Exim 4.60 #1 (Debian))\n\tid 1FMd5f-0000rJ-JS; Fri, 24 Mar 2006 03:35:55 +0000\r\nReceived: from nferrier by wikidemo with local (Exim 4.50)\n\tid 1FMd4O-0003TF-P7; Fri, 24 Mar 2006 03:34:36 +0000\r\nTo: Karl Dubost &lt;karl@...&gt;\r\nCc: rest-discuss Discuss &lt;rest-discuss@yahoogroups.com&gt;\r\nReferences: &lt;44186DDD.6010305@...&gt; &lt;442194E9.8000609@...&gt;\n\t&lt;87acbimgq2.fsf@...&gt; &lt;4421A9F8.5070503@...&gt;\n\t&lt;4421F530.8000802@...&gt; &lt;874q1qlz4c.fsf@...&gt;\n\t&lt;4422336C.5000307@...&gt; &lt;87y7z1l6co.fsf@...&gt;\n\t&lt;73ec599d0603230608p2abe514cucb451dcd244671d8@...&gt;\n\t&lt;87veu5ky0x.fsf@...&gt;\n\t&lt;73ec599d0603231343s423cbc92o468f0cef6e7e06a8@...&gt;\n\t&lt;87pskclt91.fsf@...&gt;\n\t&lt;80DF2FEB-D5F5-423F-A79E-4F64422F5198@...&gt;\n\t&lt;87lkv0lha2.fsf@...&gt;\n\t&lt;D42AD369-5724-4947-806E-6664546A791B@...&gt;\r\nDate: Fri, 24 Mar 2006 03:34:36 +0000\r\nIn-Reply-To: &lt;D42AD369-5724-4947-806E-6664546A791B@...&gt; (Karl Dubost&#39;s message of &quot;Fri, 24 Mar 2006 11:36:30 +0900&quot;)\r\nMessage-ID: &lt;87fyl8ldmb.fsf@...&gt;\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=us-ascii\r\nX-eGroups-Msg-Info: 1:12:0:0\r\nFrom: Nic &lt;nferrier@...&gt;\r\nSubject: Re: [rest-discuss] content negotiation, single resources, and the web today\r\nX-Yahoo-Group-Post: member; u=3702469; y=SLKrXWYjYbV55Rv28-ltfdEcTdlV7049cLNSZsmxkLhkMdhP4_bAhubJdtg7Kp7FEPY\r\nX-Yahoo-Profile: nferrier_tapsellferrier\r\n\r\nKarl Dubost &lt;karl@...&gt; writes:\n\n&gt;&gt;&gt; * Multiple Web sites hosted at the same domain name\n&gt;&gt;\n&gt;&gt; How do you know with robots.txt right now? You don&#39;t right?\n&gt;\n&gt; And that&#39;s bad.\n\nYes. As you said there&#39;s no way of fixing it using Well Known URIs.\n\n\n&gt;&gt; I would say that the best way to deal with robots would be to have the\n&gt;&gt; robot do content-neg on every page and for the correct content-neg\n&gt;&gt; response to be the location of the robots.txt for that &quot;site&quot;. With\n&gt;&gt; every request for robot info the cache strikes will increase.\n&gt;&gt;\n&gt; Which means User enters URI in UA.\n&gt;\n&gt; UA starts a series of GET\n&gt; \trobot\n&gt; \tp3p\n&gt; \tfavicons\n&gt; \ttags\n&gt; \tmypuppy\n&gt; \tcoolweb2.0features\n&gt; \tetc.\n&gt; \tetc.\n&gt; \tAnd finally the Web page itself\n\neh? Why would the user retrieve the robots file?\n\nAnd actually pretty much the first retrieval for a UA would be the web\npage itself because it might have markup for doing things like\nfavicon.\n\n\nBut we&#39;re talking about robots in this instance, right? Not people\nusing browsers. The interactions we&#39;re talking about are that the\nrobot does something like this:\n\n  GET / HTTP/1.1\n  Host: http://www.example.com\n  Accept: robot/robot\n\nthen\n\n  GET / HTTP/1.1\n  Host: http://www.example.com\n  Accept: *\n\na resource from there tells the robot about:\n\n   http://www.example.com/furtherpage\n   http://www.example.com/johnssite\n\nso the robot does:\n\n  GET /furtherpage HTTP/1.1\n  Host: http://www.example.com\n  Accept: robot/robot\n\nand gets nothing, so straight onto:\n\n  GET /furtherpage HTTP/1.1\n  Host: http://www.example.com\n  Accept: *\n\n\na bit later, in a different thread it deals with john&#39;s site:\n\n  GET /johnssite HTTP/1.1\n  Host: http://www.example.com\n  Accept: robot/robot\n\nand gets back a robot representation that explains how to handle\nJohn&#39;s site. Then it does:\n\n  GET /johnssite HTTP/1.1\n  Host: http://www.example.com\n  Accept: *\n\nand finishes it&#39;s trawling.\n\nUnless I really am parsing you wrong this is what you want: someway\nfor &quot;farmed&quot; sites (those not anchored at a the domain root of a uri)\nto be real sites.\n\nOf course: this is NOT how it works currenty. But IF it worked this\nway then farmed sites could be real sites whereas right now they\nreally can&#39;t (unless the HTML META tag is used but that&#39;s really just\npoor man&#39;s content neg [because it only works with HTML]).\n\n\n&gt;&gt;   a resource *could* be a &#39;site&#39; by simplying accepting neg for &#39;site&#39;\n&gt;&gt;   representations\n&gt;\n&gt; See above.\n\nSorry. Too concise for me. I don&#39;t understand what your critism\nactually _is_.\n\n\nNic\n\n"}}